---
title: "SME0806 - Estatística Computacional - Trabalho 1"
author:
  - nome 1 (nusp)
  - Francisco Rosa Dias de Miranda (4402962)
  - nome 3 (nusp)
  - nome 4 (nusp)
output: pdf_document
---

```{r packages, message = F}
library(tidyverse)
library(pander)
# Separador decimal nos resultados: ","
options(OutDec = ",")
set.seed(123)
knitr::opts_chunk$set(echo=FALSE)
```

O que falta:

- explicitar g\* em 1b:d, deixar as integrais como feito no fim do item (a)
- item 2

## Exercício 1

### a.

Para calcular a integral, primeiro reescrevemos os limites de integração com auxílio da função indicadora:

$$\Theta = \int_0^\infty \int_0^x g(x,y) dy dx = \int_0^\infty  \int_0^\infty g(x,y) \ \mathbb{I}{(y)}_{(0,x)} dy dx$$

Para ajustar os intervalos, propomos a seguinte mudança de variáveis, que mapeia a região de integração no quadrado unitário.

$$u = 1 - e^{-x},\ v = 1 - e^{-y} \Rightarrow x = -log(1 - u),\ y = -log(1 - v) $$

Note que a transformação funciona pois se $x = 0$ então $u = 0$ e se $x \rightarrow \infty$ então $u \rightarrow 1$, de forma análoga para $y$ e $v$. Calculamos $J(u,v)$ usando a matriz de derivadas parciais da transformação inversa

$$
\begin{pmatrix}
\frac{\partial u}{\partial x} & \frac{\partial u}{\partial y} \\
\frac{\partial v}{\partial x} & \frac{\partial v}{\partial y}
\end{pmatrix}
=
\begin{pmatrix}
\frac{1}{(1 - u)} & 0 \\
0 & \frac{1}{(1 - v)}
\end{pmatrix}
 \ \Rightarrow \ |J(u,v)| = \frac{1}{(1 - u)(1 - v)}
$$

Dessa forma, podemos reescrever a integral como sendo

$$\Theta = \int_0^1  \int_0^1 g(u,v) \ \mathbb{I}{(v)}_{(0, -log(1 - u))} \frac{1}{(1 - u)(1 - v)} dv du$$

Para se resolver integrais utilizando o método de Monte Carlo, é proposto o seguinte algoritmo:

1. Gere uma dupla $(u,v)$ com $u \sim U(0,1)$ e $v \sim U(0,1)$ independentes
2. Calcule $g^*(u, v) = g(u, v) * J(u, v)$
3. Faça $\hat{\theta} = \frac 1 n \sum_{j = 1}^M g_i(u, v)$

### b.

Vamos agora calcular
$$\int_0^\infty \int_0^x \exp^{-(x+y)} dydx$$

```{r}
# Numero de amostras
M <- 50000

# Gera um vetor de pares (x, y) de uniformes (0,1)
gera_pares_unif <- function(n) {
  return(tibble(
    u = runif(n),
    v = runif(n)))
}

# Funcao indicadora
ind <- function(x, min, max) {
  return(
    ifelse(x > min,
      ifelse(x < max,
             1, 0),
      0))
}

# jacobiano da transformação inversa
jac <- function(u, v) {
  return(
    1 / ((1 - u) * (1 - v)))
}

# obtem g*(.) a partir de g(.) a partir da transformação adotada
gstar <- function(g) {
  return(function(x, y) {
    g(-log(1 - x), - log(1 - y)) * ind(y, 0, x) * jac(x, y)
  })
}

## Integra uma função em (0, inf) atraves do método
## de Monte Carlo, retorna estimativa e E.P.
integral_mc_unif <- function(gstar, M) {
  return(
   gera_pares_unif(M) |>
   mutate(
     gs = gstar(u, v)) |>
   summarise(
     theta_hat = mean(gs),
     ep_mc = sqrt(var(gs) / M))
  )
}

g1 <- function(x, y) {
  return(
    exp(-(x + y)))
}
labs <- c("$\\hat{\\theta}$", "$\\widehat{Var}(\\hat{\\theta})$")

# Estimativa e EP da integral
gstar(g1) |> integral_mc_unif(M) |> knitr::kable(col.names = labs)
```

### c.

$$\int_0^\infty \int_0^x \exp^{-(x^{2}+y^{2})} dydx$$

```{r}

g2 <- function(x, y) {
  return(
         exp(-(x^2 + y^2)))
}
# Estimativa e EP da integral
gstar(g2) |> integral_mc_unif(M) |> knitr::kable(col.names = labs)

```

### d.

$$\int_0^\infty \int_0^x \exp^{-(x+y)^{2}} dydx$$

```{r }
g3 <- function(x, y) {
  return(
         exp(-(x + y)^2))
}
# Estimativa e EP da integral
gstar(g3) |> integral_mc_unif(M) |> knitr::kable(col.names = labs)

```

## Exercício 2

### a.

Vamos gerar amostras de $X$ através do _método da inversão_:

1. Gere $u \sim U(0,1)$
2. Faça $x = f^{-1}(u)$

Invertendo a função, obtemos $f^{-1}(x) = ± (3^(1/3) (-log(x))^(1/3))$.

```{r }
finv <- function(x) {
  ifelse()
   (3^(1/3) (-log(x))^(1/3))
  }
```

### b.

Para resolver a integral utilizando o método de Monte Carlo, é proposto o seguinte algoritmo:

1. Gere $u \sim N(0,1)$
2. Calcule $f^*(u) = \frac{f(u)}{g(u)}$, onde $g(.)$ é a f.d.p de uma Normal Padrão
3. Faça $\hat{\theta} = \frac 1 n \sum_{j = 1}^M f^{*}_i\*(u)$

```{r }

# Obtendo estimativas de MC para diferentes valores de R
R <- c(seq(5, 1000, 20), seq(2000, 1e5, 5000))

u <- R |> map_dfr(~tibble(x = rnorm(.),
                      R = .))

fstar <- function(x){
  return(
    exp(- abs(x)^3 / 3) / dnorm(x))
  #         f(x)       /   g(x)
}

tabela <- u |>
  mutate(fs = fstar(x)) |>
  group_by(R) |>
  summarise(theta_hat = mean(fs),
            ep = sd(fs) / n() )


tabela |>
  ggplot(aes(x = R)) +
  geom_point(aes(y = theta_hat)) +
  geom_errorbar(aes(ymin = theta_hat - ep, ymax = theta_hat + ep), width =.1) +
  geom_hline(aes(yintercept = 2.5758), ) +
  scale_x_log10()

```

```{r }
R <- 50000

fstar <- function(x) {
  return(
    x^2 * exp(- abs(x)^3 / 3) / dnorm(x))
  #         f(x)       /   g(x)
}

tabela <- u |>
  mutate(fs = fstar(x)) |>
  group_by(R) |>
  summarise(theta_hat = mean(fs),
            ep = sd(fs) / n() )
```

# Apêndice: códigos utilizados na análise

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}

```

```{r }


```
