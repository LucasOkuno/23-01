---
title: "SME0806 - Estatística Computacional - Trabalho 1"
author:
- Alvaro Valentim (10392150)
- Francisco Rosa Dias de Miranda (4402962)
- Victor Botelho Cardoso (11953925)
- nome 4 (nusp)
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r packages, message = F, echo = F}
library(tidyverse)
library(knitr)
# Separador decimal nos resultados: ","
options(OutDec = ",")
set.seed(123)
knitr::opts_chunk$set(echo=FALSE)
```


O que falta:

- explicitar g\* em 1(b)(c)(d) deixar as integrais como feito no fim do item (a), e tambem verdadeiros valores das integrais
- item 2: escrever f(.) direito,

## Exercício 1

### a.

Para calcular a integral, primeiro reescrevemos os limites de integração com auxílio da função indicadora:

$$
  \theta = \int_0^\infty \int_0^x g(x,y) dy dx = \int_0^\infty  \int_0^\infty g(x,y) \ \mathbb{I}{(y)}_{(0,x)} dy dx \ \ \ \ \ (1)
$$

Para ajustar os intervalos, propomos a seguinte mudança de variáveis, que mapeia a região de integração no quadrado unitário

$$
u = 1 - e^{-x},\ v = 1 - e^{-y} \Rightarrow x = -log(1 - u),\ y = -log(1 - v) \ \ \ \ \ (2)
$$

Note que a transformação funciona pois se $x = 0$ então $u = 0$ e se $x \rightarrow \infty$ então $u \rightarrow 1$, de forma análoga para $y$ e $v$. Calculamos $J(u,v)$ usando a matriz de derivadas parciais da transformação inversa

$$
J(u,v) =
\begin{pmatrix}
\frac{\partial x}{\partial u} & \frac{\partial y}{\partial u} \\
\frac{\partial x}{\partial v} & \frac{\partial y}{\partial v}
\end{pmatrix}
=
\begin{pmatrix}
\frac{1}{(1 - u)} & 0 \\
0 & \frac{1}{(1 - v)}
\end{pmatrix}
 \ \Rightarrow \ |J(u,v)| = \frac{1}{(1 - u)(1 - v)}
$$

Dessa forma, reescrevemos a integral no novo sistema de coordenadas

$$\theta = \int_0^1  \int_0^1 g(u,v) \ \mathbb{I}{(v)}_{(0, u)} |J(u,v)| dv du$$

Como estamos utilizando a distribuição Uniforme(0,1) temos que $g^{*}(u, v) = g(u, v) * |J(u, v)|$. Podemos agora resolver as integrais via método de Monte Carlo através do seguinte algoritmo:

1. Gere duplas $(u_{i},v_{i})$ com $u \sim U(0,1)$ e $v \sim U(0,1),\ i = 1, ..., R$
2. Calcule $g^*(u, v)$
3. Faça $\hat{\theta} = \frac 1 n \sum_{j = 1}^M g_i(u, v)$

Com auxílio de software computacional, escrevemos um programa em linguagem R para resolver as integrais com 50000 amostras de pares da distribuição Uniforme(0,1) disponível no Apêndice.

```{r }

# Numero de amostras
M <- 50000

# Gera um vetor de pares (x, y) de uniformes (0,1)
gera_pares_unif <- function(n) {
  return(tibble(
    u = runif(n),
    v = runif(n)))
}

# Funcao indicadora
ind <- function(x, min, max) {
  return(
    ifelse(x > min,
      ifelse(x < max,
             1, 0),
      0))
}

# jacobiano da transformação inversa
jac <- function(u, v) {
  return(
    1 / ((1 - u) * (1 - v)))
}

# obtem g*(.) a partir de g(.) usando a transformação exponencial
gstar <- function(g, x, y) {
  return(function(x, y) {         ##  y em (0, x)
    g(-log(1 - x), - log(1 - y)) * ind(y, 0, x) * jac(x, y)
  })
}

## Integra uma função em y \in (0, x), x \in (0, inf)  atraves
## do método de Monte Carlo, retorna estimativa e E.P.
integral_mc_unif <- function(gstar, R = 50000) {
  return(
   gera_pares_unif(R) |>
   mutate(
     gs = gstar(u, v)) |>
   summarise(
     theta_hat = mean(gs),
     ep_mc = sqrt(var(gs) / R))
  )
}

```

### b.

Vamos transformar $g(x,y)$ em $g(u,v)$, para isso, utilizaremos as transformações em $(2)$.
$$
g(x,y) = exp^{-(x+y)} \Rightarrow g(u,v) = exp^{-(-log(1-u)-log(1-v))} = exp^{log(1-u)} exp^{log(1-v)} = (1-u)(1-v)
$$
Agora, substituindo em $(1)$, temos:
$$\int_0^\infty \int_0^x \exp^{-(x+y)} dydx = \int_0^1  \int_0^1 (1-u)(1-v) \ \mathbb{I}{(v)}_{(0, u)} \frac{1}{(1 - u)(1 - v)} dv du$$
```{r}
g1 <- function(x, y) {
  return(
    exp(-(x + y)))
}
labs <- c("$\\hat{\\theta}$", "$\\widehat{Var}(\\hat{\\theta})$")

# Estimativa e EP da integral
gstar(g1) |> integral_mc_unif() |> kable(col.names = labs)
```

Considerando o valor real de $1/2$, conseguimos chegar relativamente próximo com esse método de aproximação, porém dependendo do caso, pode não ser satisfatório.

### c.
Repetindo o mesmo processo para o item $(c)$, com base na tranformação em $(2)$, temos:

$$g(x,y) = exp^{-(x^2+y^2)} \Rightarrow g(u,v) = exp^{-((-log^2(1-u))+(-log^2(1-v)))}$$

$$
\begin{align}
  g(x,y) = exp^{-(x^2+y^2)} \Rightarrow g(u,v) &= exp^{-((-log^2(1-u))+(-log^2(1-v)))} \\
        &= exp^{-( log^2(1-u) + log^2(1-v) )} \\
        &= exp^{-log^2(1-u)}  exp^{-log^2(1-v)} \\
        &= (1-u)^{-2}(1-v)^{-2}
\end{align}
$$
Logo,

$$\int_0^\infty \int_0^x \exp^{-(x^{2}+y^{2})} dydx = \int_0^1  \int_0^1 (1-u)^{-2}(1-v)^{-2} \ \mathbb{I}{(v)}_{(0, u)} \frac{1}{(1 - u)(1 - v)} dv du $$

```{r}

g2 <- function(x, y) {
  return(
         exp(-(x^2 + y^2)))
}
# Estimativa e EP da integral
gstar(g2) |> integral_mc_unif() |> knitr::kable(col.names = labs)

```

Vemos que podemos nos aproximar relativamente do resultado de $\pi/8 \approx 0.3927$, mas como no caso anterior, dependendo do problema, talvez não seja próximo o suficiente. 

### d.

Utilizando como base a resposta do item $(b)$, temos:

$$\int_0^\infty \int_0^x \exp^{-(x+y)^{2}} dydx = \int_0^1  \int_0^1 [(1-u)(1-v)]^{-2} \ \mathbb{I}{(v)}_{(0, u)} \frac{1}{(1 - u)(1 - v)} dv du$$


```{r }
g3 <- function(x, y) {
  return(
         exp(-(x + y)^2))
}
# Estimativa e EP da integral
gstar(g3) |> integral_mc_unif() |> knitr::kable(col.names = labs)

```

Considerando a resposta de $1/4$, nossa modelo se aproxima bem, porém pode não ser próximo o suficiente dependendo do caso, assim como nos demais itens.

## Exercício 2

### a.

Vamos gerar amostras de $X$ através do _método da inversão_:

1. Gere $u \sim U(0,1)$
2. Faça $x = f^{-1}(u)$

```{r}
f <- function(x) exp(-abs(x)^3/3)
curve(f, -5, 5, xlab = "y", ylab = "g(y)", col = "red", lwd = 2)
```

Invertendo a função, obtemos $f^{-1}(x) = ± (\sqrt[3]{3} (\sqrt[3]{-log(x)})$.

```{r }
finv <- function(x) {
  ifelse(x > 0, (3^(1/3))*(-log(x))^(1/3), -(3^(1/3))*(-log(x))^(1/3)) ## escrever o modulo
  }
```
```{r}
set.seed(12345)
n1 <- 1000
n2 <- 5000
n3 <- 10000
x1 <- sapply(runif(n1), finv)
x2 <- sapply(runif(n2), finv)
x3 <- sapply(runif(n3), finv)
```
Histograma para amostra com tamanho 1000.

```{r}
hist(x1, breaks = 30, main = paste("Amostra com n =", n1))
```
Histograma para amostra tamanho 5000.

```{r}
hist(x2, breaks = 30, main = paste("Amostra com n =", n2))
```
Histograma para amostra tamanho 10000.

```{r}
hist(x3, breaks = 30, main = paste("Amostra com n =", n3))
```

### b.

Para resolver a integral utilizando o método de Monte Carlo, é proposto o seguinte algoritmo:

1. Gere $u \sim N(0,1)$
2. Calcule $f^*(u) = \frac{f(u)}{g(u)}$, onde $g(.)$ é a f.d.p de uma Normal Padrão
3. Faça $\hat{\theta} = \frac 1 n \sum_{j = 1}^M f^{*}\*(u_i)$

```{r }

# Obtendo estimativas de MC para diferentes valores de R
R <- c(seq(5, 1000, 20), seq(2000, 1e5, 5000))

u <- R |> map_dfr(~tibble(x = rnorm(.),
                      R = .))

fstar <- function(x){
  return(
    exp(- abs(x)^3 / 3) / dnorm(x))
  #         f(x)       /   g(x)
}

tabela <- u |>
  mutate(fs = fstar(x)) |>
  group_by(R) |>
  summarise(theta_hat = mean(fs),
            ep = sd(fs) / n() )


tabela |>
  ggplot(aes(x = R)) +
  geom_point(aes(y = theta_hat)) +
  geom_errorbar(aes(ymin = theta_hat - ep, ymax = theta_hat + ep), width =.1) +
  geom_hline(aes(yintercept = 2.5758), ) +
  scale_x_log10()

```

```{r }
R <- 50000

fstar <- function(x) {
  return(
    x^2 * exp(- abs(x)^3 / 3) / dnorm(x))
  #         f(x)       /   g(x)
}

tabela <- u |>
  mutate(fs = fstar(x)) |>
  group_by(R) |>
  summarise(theta_hat = mean(fs),
            ep = sqrt(var(fs) / n() ))

```

# Apêndice: códigos utilizados na análise

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}

```


$$
